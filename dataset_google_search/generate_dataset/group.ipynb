{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "824\n",
      "[6, 67, 100, 148, 162, 203, 212, 213, 227, 235, 258, 260, 304, 319, 327, 383, 409, 410, 426, 460, 543, 550, 570, 575, 585, 599, 646, 661, 684, 689, 707, 736, 755, 767, 775, 776, 799, 807]\n",
      "786\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "files = glob('./dataset_google_search/generate_dataset/eventsource*.csv')\n",
    "# print(fles)\n",
    "df = pd.concat((pd.read_csv(file, usecols=['keyword','rank','domain_page','url','title'], dtype={ 'keyword':str,'rank':str,'domain_page': str, 'url':str,'title':str}) for file in files), ignore_index=True)\n",
    "# print(df)\n",
    "newdf= df.sort_values(by='domain_page')\n",
    "print(len(newdf))\n",
    "urls = []\n",
    "collect_idx = []\n",
    "for idx, url in enumerate(newdf['url']):\n",
    "    if url not in urls:\n",
    "       urls.append(url)\n",
    "    else:\n",
    "        collect_idx.append(idx)\n",
    "print(collect_idx)\n",
    "df_transpose = newdf.T\n",
    "for idx in reversed(collect_idx):\n",
    "    df_transpose.pop(idx)\n",
    "newdf = df_transpose.T\n",
    "print(len(newdf))\n",
    "newdf.to_csv('./dataset_google_search/generate_dataset/google_search_new.csv',index=False,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "   rank                domain_page  \\\n",
      "id                                   \n",
      "39    4    https://www.twtc.com.tw   \n",
      "40   27  https://www.yatsen.gov.tw   \n",
      "41    1                        NaN   \n",
      "42   22                        NaN   \n",
      "43   15                        NaN   \n",
      "\n",
      "                                                  url  \n",
      "id                                                     \n",
      "39    https://www.twtc.com.tw/exhibition.aspx?p=menu1  \n",
      "40         https://www.yatsen.gov.tw/content_282.html  \n",
      "41  https://www.beclass.com/default.php?name=ShowL...  \n",
      "42  https://www.npm.gov.tw/Exhibition-Preview.aspx...  \n",
      "43  https://event.moc.gov.tw/sp.asp?xdurl=HySearch...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./dataset_google_search/generate_dataset/google_data_rank - train final.csv',usecols=['rank','domain_page','url'], dtype={ 'rank':str,'domain_page': str, 'url':str})\n",
    "# print(len(df))\n",
    "df= df.sort_values(by='domain_page')\n",
    "df.index = pd.RangeIndex(start=1,stop=44,name='id')\n",
    "print(len(df.domain_page))\n",
    "print(df.tail())\n",
    "df.to_csv('./dataset_google_search/generate_dataset/pick_rank/google_search_rank_train.csv',encoding='utf_8_sig')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rank                      domain_page  \\\n",
      "id                                         \n",
      "1    33         https://arts.nycu.edu.tw   \n",
      "2    35   https://asiamodern.asia.edu.tw   \n",
      "3    53  https://coach.taiwanjobs.gov.tw   \n",
      "4    34          https://cte.nptu.edu.tw   \n",
      "5    57     https://ihuodong.ntue.edu.tw   \n",
      "\n",
      "                                                  url  \n",
      "id                                                     \n",
      "1   https://arts.nycu.edu.tw/arts-lectures/hao-ran...  \n",
      "2   https://asiamodern.asia.edu.tw/zh_tw/exhibitio...  \n",
      "3   https://coach.taiwanjobs.gov.tw/wdaecPublic/Ap...  \n",
      "4   https://cte.nptu.edu.tw/p/412-1023-74.php?Lang...  \n",
      "5             https://ihuodong.ntue.edu.tw/category/8  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./dataset_google_search/generate_dataset/google_search_dataset/google_data_rank - vali final.csv',usecols=['rank','domain_page','url'], dtype={ 'rank':str,'domain_page': str, 'url':str})\n",
    "# print(len(df))\n",
    "df= df.sort_values(by='domain_page')\n",
    "df.index = pd.RangeIndex(start=1,stop=21,name='id')\n",
    "print(df.head())\n",
    "df.to_csv('./dataset_google_search/generate_dataset/pick_rank/google_search_rank_vali.csv',encoding='utf_8_sig')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561\n",
      "361\n",
      "count_not_event:181,count_error_url:19\n",
      "precission:0.64349376114082,remove error url precision:0.6660516605166051\n"
     ]
    }
   ],
   "source": [
    "# 第一次過濾\n",
    "# import pandas as pd\n",
    "# google_search=pd.read_csv('google_search.csv')\n",
    "# print(len(google_search))\n",
    "# collect_event_idx = []\n",
    "# count_not_event = 0\n",
    "# count_error_url = 0\n",
    "# for idx, label in enumerate(google_search['Label']):\n",
    "#     if label == 1:\n",
    "#         collect_event_idx.append(idx)\n",
    "#     elif label == 0:\n",
    "#         count_not_event+=1\n",
    "#     elif label == -1:\n",
    "#         count_error_url+=1\n",
    "# print(len(collect_event_idx))\n",
    "# print('count_not_event:{},count_error_url:{}'.format(count_not_event,count_error_url))\n",
    "# print('precission:{},remove error url precision:{}'.format(len(collect_event_idx)/len(google_search),len(collect_event_idx)/(len(google_search)-count_error_url)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# new_google_search =google_search.iloc[collect_event_idx,:]\n",
    "# new_google_search.to_csv('google_search_event.csv',index=False,encoding='utf_8_sig')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361\n",
      "202\n",
      "count_not_event_source:159\n",
      "precission:0.5595567867036011\n"
     ]
    }
   ],
   "source": [
    "# 第二次過濾\n",
    "# import pandas as pd\n",
    "# google_search_event=pd.read_csv('google_search_event.csv')\n",
    "# print(len(google_search_event))\n",
    "# collect_event_source_idx = []\n",
    "# count_not_event_source = 0\n",
    "# for idx, label in enumerate(google_search_event['Label']):\n",
    "#     if label == 1:\n",
    "#         collect_event_source_idx.append(idx)\n",
    "#     elif label == -2:\n",
    "#         count_not_event_source+=1\n",
    "# print(len(collect_event_source_idx))\n",
    "# print('count_not_event_source:{}'.format(count_not_event_source))\n",
    "# print('precission:{}'.format(len(collect_event_source_idx)/len(google_search_event)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}